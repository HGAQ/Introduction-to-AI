# AI引论

## Chap01 概论

## Chap02 数学基础

- **样本空间&样品点**
  - **样本空间**：所有可能的结果
  - **样本点**：样本空间元素
- **随机事件**
  - 满足某些条件的样本点组成的集合。它是样本空间 Ω 的子集，记为 𝐴,𝐵, ...
  - 关系
    - 包含，并，交，对立（是和不是），互斥（红灯和绿灯（还有黄灯））
- **古典概型**
  - **有限性**：只有有限个试验结果（样本）
  - **等可能性**：每个试验结果（样本）在一次试验中出现的可能性相等；
- **条件概率**
  - 在事件𝐵已经出现的条件下，事件𝐴发生的概率，记作 P(A | B).
  - **P(A | B) = P(A ∩ B) / P(B)** => **P(A ∩ B)=P(A | B) · P(B)**
- **独立事件**
  - 若 P(AB) = P(A) · P(B),则**相互独立**。
  - 若 P(ABCDEFG) = P(A) P(B) …… P(G)，则**相互独立**。
  - 若 P(AB) = P(A) · P(B)；P(CB) = P(C) · P(B)；P(AC) = P(A) · P(C)，则**两两独立**。
  - 两两独立的事件组**不一定**相互独立。
  - 相互独立的事件组**一定**两两独立。
- **全概率公式**
  - ![1](https://pic.imgdb.cn/item/63f71c5ff144a010073f3d26.jpg)
  - ![2](https://pic.imgdb.cn/item/63f71c82f144a010073f73f4.jpg)
  - ![3](https://pic.imgdb.cn/item/63f71cd1f144a010073ff0c5.jpg)
- **贝叶斯公式**
  - ![4](https://pic.imgdb.cn/item/63f72553f144a010074e3223.jpg)
- **随机变量**
  - ![5](https://pic.imgdb.cn/item/63f751c8f144a0100797e05b.jpg)
  - ![6](https://pic.imgdb.cn/item/63f751f2f144a01007981ade.jpg)
  - ![7](https://pic.imgdb.cn/item/63f7520af144a01007983a5b.jpg)
- **数学期望**
  - ![8](https://pic.imgdb.cn/item/63f75234f144a0100798720b.jpg)
  - ![9](https://pic.imgdb.cn/item/63f7524bf144a01007989334.jpg)
- **方差与标准差**
  - ![10](https://pic.imgdb.cn/item/63f7525ff144a0100798b02d.jpg)
  - 方差的算术平方根 $\sqrt{𝐷(X)}$ 称为 X 的标准差或均方差，记为σ (X)
- **协方差**
  - 若随机变量 𝑋 的期望 𝐸(𝑋) 和 𝑌 的期望 𝐸(𝑌) 存在，则称
  - $$Cov(X,Y)=E{[X-E(x)\][Y-E(Y)]}$$
    为 𝑋 与 𝑌 的协方差。
  - 若随机变量 𝑋，𝑌的方差和协方差均存在，且 𝐷(𝑋) > 0，𝐷(𝑌) > 0，则
  - $$ρ(X,Y)=\frac{Cov(X,Y)}{\sqrt{D(X)·D(Y)}}$$称为 𝑋，𝑌 的相关系数。

## Chap03 python

- intended left blank

## Chap04-05 搜索

- 目标（goal）：即我们要去“找”什么；什么时候可以结束搜索
- 状态（state）：这里面其实包括三个主要的部分，开始状态（initial states），目标状态（goal states），当前状态（current state）
- 动作（actions）：智能体可以采取的行动/决策
- （状态）转移模型（transition model）：当前状态随着动作会怎么变化
- 动作代价函数（action cost function）：每个动作要消耗多大的成本
- 深度优先搜索：DFS
  - 一条路走到黑，不撞南墙不回头
  - stack！！！（后进先出）
  - 不是代价最小的，也不是最短的
- 广度优先搜索：BFS
  - 搜索一步的所有情况
  - queue！！！（先进先出）
  - 最短但是不是代价最小的
- 一致代价搜索：UFS
  - 我们不应该只关注单边的成本，而是要考虑总成本（从开始节点累积到当前节点），广搜是UFS的一种
  - 代价->步骤
  - 代价轮廊
  - priority queue！！（优先队列）
    - 保存所有节点与优先值
    - 优先队列通常依靠 堆（heap）来实现
    - 依靠树的层级来维持顺序，父母节点一定小于等于孩子节点（树是完整的，即只有最底层的右边有空缺）
  - 假设最佳答案是有限代价，并且边的最低代价是正的，算法完备且最优。
  - **没有考虑关于目标的信息**
- 启发算法
  - 启发(heuristic)是一个估计当前状态离目标状态还有多“远”的方程
- 贪心搜索
  - 永远扩展看起来最近的（启发告诉我们离目标最近的节点）
  - 不是最优
  - 最差情况: 像一个被错误引导的深搜，最后导致要扩展完整个搜索空间才能到目标
- A*
  - UCS 向后看，根据至今积累的代价排序 g(n)
  - 贪心 向前看, 根据离目标还有多远的估计排序 h(n)
  - A* 根据两者的和决定顺序 : f(n) = g(n) + h(n)
  - 一个启发 ℎ 是 可接受的 (乐观的)，需满足:
    - 启发小于代价，单边的启发值小于实际动作代价
    - 从而，在一条路径下f永远不会减小
  - $$ 0≤h(n)≤h^*(n)$$

$$ g(E) = \frac{8\pi}{3h^3}(2m)^{3/2}E^{1/2} $$

## Chap06 逻辑与CSP

- 变量、域、约束、目标
- 回溯搜索：
  - 先考虑一个变量，然后每一步判断若不符合就回退。
  - 25皇后
- 改进：
  - 顺序：
    - 最少剩余值启发：选择可能性最少的变量赋值
    - 当一个变量被赋值的时候，选择给剩下的变量留下最多选择的值。
  - 筛选：
    - 约束传递: 从约束推理到约束
    - 一条边 X → Y 是一致的：对于每一个X的剩余值，Y都有某个赋值方式，使其不会违反约束
- SAT问题：是否存在一种布尔赋值组合，使所有的逻辑约束都能被满足
  - 将问题变为布尔运算
  - DPLL算法：
    - 合取范式（CNF）： (p1∨¬p3∨p4 ) ∧ (¬p1∨p2∨¬p3 ) ∧ ...
      - 就是每一个都要真
    - 单字符传递（unit propagation）：当某个子句只剩下一个字符，对这个变量赋值使子句为真
    - 布尔约束传递(BCP):重复使用单字符传递，直到无法使用为止
    - （就是深搜……
  - 矛盾指引的子句学习算法:
    - 隐含图（implication graph）：对于SAT的搜索树，我们有两种主要赋值方式，手动赋值，和由BCP产生的赋值。
    - 通过已有的条件推出越来越多的矛盾语句
    - 对于完全可观察的、确定性的情况:
      - 规划问题是可解的 当且仅当 存在可满足的赋值
      - 解法从动作变量的赋值获得

## Chap07 对抗博奕

- 零和博弈(Zero-Sum Games )
  - 效用：关于回报大小的度量
  - 智能体的效用是对立的，比如：当一个智能体最大化其效用时，另一智能体效用为其最小值
  - 特点：对抗的(adversarial), 完全竞争(purecompetition)
- 一般博弈(General Games)
  - 智能体获得的效用是独立的
- 搜索：代价(cost) -> 效用(utility)!
  - 状态价值: 从该状态出发可能获得的最大最终效用
  - 效率如何？
    - 与DFS一致
    - 时间复杂度: 𝑂(𝑏^𝑚)
    - 空间复杂度: 𝑂(𝑏m)
- 确定性零和博弈:
  - 井字棋, 国际象棋, 跳棋、围棋
  - 其中一个玩家最大化价值，对手玩家最小化价值

- 极小极大搜索:
  - 状态空间的搜索树
  - 玩家轮流行动
  - 计算每个节点极大极小价值: 假设对手是理性的情况下（即对方总是努力达到最优），自己可以达到的最优效用值

## Chap08 蒙特卡洛搜索

- 蒙特卡洛搜索（MCTS）
  - 利用（大规模）随机抽样来近似问题的解
  - 20世纪40年代，一群从事核弹制造的科学家以蒙特卡洛赌场命名
  - 次数越多越准确
    - 估计的准确度与样本方差有关
    - ![mtkl](https://pic.imgdb.cn/item/6417f363a682492fcc72ccb3.jpg)
- 𝜀-greedy
  - 1- 𝜀的时候：选当前最优的
  - 𝜀的时候：随机选取一个当前不是最优的
    - 如果我们可以更多地去尝试那些更有希望成为更优的动作，算法的效率将更高
- 最大置信（UCB）
  - 尝试次数越多估值越准确，也即我们对尝试次数少的估计信任不足
  - 算法应平衡 估值的大小 和 次数的多少
  - $$A_{t+1}=argmax_a[Q_t(a)+c\sqrt{\frac{lnt}{N_t(a)}}]$$
  - 𝑐: 超参数，用于平衡估值和信任度对决策的影响
- 博弈
  - ![mtkl2](https://pic.imgdb.cn/item/6417f8c4a682492fcc7e5c8f.jpg)
  - 选择 扩展 模拟 回溯
  - 选择:
    - 每个圆圈都包含获胜次数 / 通过该节点次数
      - 在根节点，执行UCB
        - （c设为0.5）
        - 自左至右：1.05 0.97 0.67
      - 在2/3节点，执行UCB
        - 自左至右：0.52 0.87
  - 扩展：
    - 选择一次随机移动并扩展一个新的节点（加粗），初始化为0/0
  - 模拟：
    - 在实际使用MCTS的时候，模拟经常不会选择一直运行到博弈结束。而是会设设置一个最大步数，然后直接返回启发值。
  - 回溯：
    - 在模拟结束后，所采取路径上的所有记录都会被更新
    - 每个相关节点的次数都会加1，如果最后获胜了每个相关节点都会将胜利次数加1，这在加粗的数字中显示
  - 再次选择………………
- 